\documentclass[UTF8]{article}%字符标准，纸张，字体大小，文档类型，自行打印11pt，单位打印12pt
\usepackage{mathtools,amssymb,array,amsthm,amsmath}%数学宏包
\usepackage{geometry,ulem,graphicx,longtable,caption2,cite,fancyhdr,multicol,color}%通用宏包
\geometry{a4paper,left=2.5cm, right=2.5cm, top=2.6cm, bottom=3cm}%页面布局
\usepackage{ctex}
\usepackage{multirow} % Required for multirows
\newcommand\dd{\mathop{}\!\mathrm{d}}%定义微分算子
\newcommand\eu{\mathrm{e}}%定义自然常数
\newcommand\cbnt{\mathrm{C}}%定义自然常数\\
\newcommand\argm{\mathrm{A}}%定义自然常数
\renewcommand\bar{\overline}
\renewcommand\vec{\overrightarrow}
\usepackage{graphicx,graphics}
\usepackage{wrapfig}
\usepackage{xeCJK}
\usepackage{cite}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\xeCJKsetup{
	CJKecglue={\:}
}
\AtBeginDocument{
	\let\mathbb\relax
	\DeclareMathAlphabet{\mathbb}{U}{msb}{m}{n}
}
\setlength{\lineskip}{8pt}
\setlength{\lineskiplimit}{8pt}




\usepackage{listings}
\usepackage{xcolor}
\lstset{
	basicstyle=\small\ttfamily,	% 基本样式
	keywordstyle=\color{blue!70!green}, % 关键词样式
	commentstyle=\color{yellow!30!green!70},   	% 注释样式
	stringstyle=\color{red!70!purple}, 	% 字符串样式
	backgroundcolor=\color{gray!5},     % 代码块背景颜色
	frame=leftline,						% 代码框形状
	framerule=18pt,%
	rulecolor=\color{purple!10!blue!10},      % 代码框颜色
	numbers=left,				% 左侧显示行号往左靠, 还可以为right ，或none，即不加行号
	numberstyle=\footnotesize,	% 行号的样式
	firstnumber=1,
	stepnumber=1,                  	% 若设置为2，则显示行号为1,3,5
	numbersep=7pt,               	% 行号与代码之间的间距
	aboveskip=.25em, 			% 代码块边框
	showspaces=false,               	% 显示添加特定下划线的空格
	showstringspaces=false,         	% 不显示代码字符串中间的空格标记
	keepspaces=true, 					
	showtabs=false,                 	% 在字符串中显示制表符
	tabsize=2,                     		% 默认缩进2个字符
	captionpos=n,                   	% 将标题位置设置为底部
	flexiblecolumns=true, 			%
	breaklines=true,                	% 设置自动断行
	breakatwhitespace=false,        	% 设置自动中断是否只发生在空格处
	breakautoindent=true,			%
	breakindent=1em, 			%
	title=\lstname,				%
	escapeinside=``,  			% 在``里显示中文
	xleftmargin=1.5em,  xrightmargin=0em,     % 设定listing左右的空白
	aboveskip=1ex, belowskip=1ex,
	framextopmargin=1pt, framexbottommargin=1pt,
    abovecaptionskip=-2pt,belowcaptionskip=3pt,
	% 设定中文冲突，断行，列模式，数学环境输入，listing数字的样式
	extendedchars=false, columns=flexible, mathescape=true,
	texcl=true,
	fontadjust
}%

\allowdisplaybreaks[4]






\begin{document}

	\setlength{\lineskip}{8pt}
	\setlength{\lineskiplimit}{8pt}
    
\begin{table*}[!ht]
    \renewcommand\arraystretch{1.5}
    \centering  
    \begin{tabular}{|p{3.5cm}<{\centering}|p{3.5cm}<{\centering}|}
        \hline
        {队伍编号}&MC2406053\\
        \hline
        题号&C\\
        \hline        
    \end{tabular}
\end{table*}

\setcounter{page}{1}
\pagenumbering{roman}

\noindent\rule{\linewidth}{1pt}
\begin{center}
    \Large 基于随机森林和PuLP的物流预测和人员安排
\end{center}
\begin{center}
    \large\bf 摘要
\end{center}

本研究针对电子商务物流网络中分拣中心的货量预测及人员排班问题，提出一种基于{\bf 随机森林算法}与{\bf PuLP线性规划}的混合方法。研究的初衷是通过精确预测每个分拣中心在未来一段时间内的货量，并基于这些预测结果有效地安排人员，从而优化物流操作效率和降低成本。

电子商务的高速发展对物流系统提出了{\bf 更高的效率和响应速度要求}。分拣中心作为物流网络的关键节点，承担着将包裹正确快速地分类并运往下一站的任务，其运作效率直接影响整个物流网络的性能。正确预测分拣中心的货量并基于这些预测进行高效的人员安排，是管理者面临的一大挑战。

在本研究中，我们首先分析了包括过去四个月每天货量和前三十天每小时货量的历史数据，采用随机森林算法来预测接下来三十天内57个分拣中心的每天及每小时的货量。随机森林被选中是因为其在多个预测模型比较中表现出{\bf 最佳的准确性和稳健性}。通过这种方式，我们能够处理大规模数据集并提取出影响货量变化的关键因素。

接下来，利用PuLP优化库，我们根据预测的货量需求，设计并实现了一个人员排班模型。这一模型不仅要满足各个班次的货量处理需求，还要在此基础上尽可能地减少总人天数，同时确保每名员工的{\bf 工作量平衡}。此外，我们考虑到正式员工与临时工的不同成本和工作效率，优先使用正式员工，并在必要时添加临时工以满足额外的需求。

此外，研究还考虑了运输线路的调整对货量预测的影响。通过分析运输线路变更的数据，我们调整了预测模型以反映这些变化，进一步提高预测的准确性。

研究结果表明，所提出的方法能够有效预测分拣中心的货量，并据此制定出人员{\bf 成本效益高、响应迅速的排班计划}。这不仅优化了物流网络的运作效率，还为管理者提供了一个强大的决策支持工具，帮助他们在快速变化的市场环境中作出更好的资源配置决策。

最终，本文提出的综合方法为电子商务物流网络中的{\bf 货量预测与人员排班问题}提供了一种新的解决方案，展示了{\bf 机器学习与运筹学}结合的潜力，对提升物流效率和降低运营成本具有重要的理论和实际意义。



\noindent{\bf 关键词：电商物流、分拣中心、货量预测、随机森林算法、人员排班、PuLP优化、机器学习，运输线路分析。}


\newpage

\tableofcontents


\newpage
\setcounter{page}{1}
\pagenumbering{arabic}


\section{问题重述}
\subsection{问题背景}
随着网购的流行，电商物流显得更加重要。在电商物流网络中，订单配送的过程包括多个环节，其中核心环节之一是分拣。分拣中心负责根据不同的目的地对包裹进行分类，并将它们发送到下一个目的地，最终交付给顾客。因此，提高分拣中心的管理效率对整个网络的订单交付效率和成本控制至关重要。

货量预测在电商物流网络中扮演着至关重要的角色。准确预测分拣中心的货物量是后续管理和决策的基础。通常，货量预测是根据历史货物量、物流网络配置等信息，来预测每个分拣中心每天和每小时的货物量。

分拣中心的货量预测与网络的运输线路密切相关。通过分析各线路的运输货物量，可以确定各分拣中心之间的网络连接关系。当线路关系发生变化时，可以根据调整信息来提高对各分拣中心货量的准确预测。

基于货量预测的人员排班是下一步需要解决的重要问题。分拣中心的人员包括正式员工和临时工两种类型。合理安排人员旨在完成工作的前提下尽可能降低人力成本。根据物流网络的情况，制定了人员安排的班次和小时人效指标。在确定人员安排时，优先考虑使用正式员工，必要时再增加临时工。

\subsection{问题重述}
\subsubsection{问题一：建立货量预测模型}
根据提供的历史数据建立一个模型，以预测57个分拣中心在未来30天内每天以及每小时的货量。数据来源包括过去四个月的每天货量和前三十天的每小时货量。预测的准确性是优化分拣中心运营效率的关键，它直接影响到资源的配置和人员的排班计划。

\subsubsection{问题二：考虑运输线路调整的货量预测}
继问题一之后，问题二进一步要求我们在考虑运输线路变化的情况下预测货量。附件中提供了过去90天的运输线路平均货量和未来30天预期的运输线路变化。运输线路的变化可能会显著影响各分拣中心的货量，因此，我们需要调整我们的预测模型以反映这些变化。这可能涉及到重新评估与特定线路相关的历史数据的权重，或者直接在模型中引入运输线路作为一个新的解释变量。通过这种方式，我们可以更准确地预测出受线路变更影响的货量变动，从而为人员调度和资源分配提供更为精确的数据支持。


\subsubsection{问题三：基于货量预测的人员排班}

问题三要求我们根据问题二中的货量预测结果，建立一个模型来安排未来30天内每个分拣中心每个班次的人员（包括正式工和临时工）。每个分拣中心每天有六个班次，人员配置必须足以处理预测的货量，同时尽量减少总人天数。这要求我们在确保每个班次有足够人手处理预定货量的同时，还要优化人员的总工作量，避免不必要的劳动力浪费。

\subsubsection{问题四：特定分拣中心的排班优化}
问题四针对特定的分拣中心（如SC60），要求建立一个详细的排班模型，安排200名正式工在未来30天内的班次出勤计划，同时考虑雇佣临时工的需要。另外还要求每名正式工的出勤率不能超过85\%，且连续出勤天数不能超过7天。在保证分拣中心运营效率的同时，如何合理安排人员以保持工作负荷的平衡和公平是重要的考量指标。

\subsection{总体思路分析}
\subsubsection{模型假设}

\begin{enumerate}
    \item 假设一：2023-12-01 至 2023-12-31 期间不会发生如购物节等可能导致货物量激增的特殊事件。
    \item 假设二：每个分拣中心的货运量定义为在给定历史时间段中的平均值。对于每个分拣中心，平均发出货量和平均到达货量对于线路的变化应不敏感。
    \item 假设三：对于同一到达分拣中心，不同始发分拣中心对其影响程度与两者之间的货量呈正相关。
    \item 假设四：不考虑天气、交通情况等因素对分拣中心的影响。
    \item 假设五：不考虑运输过程中损坏、丢失的情况对分拣中心货量的影响。
    \item 假设六：每个数据点（每天的货量记录）被假设为独立同分布的。即假设前一天的货量与其他天的货量无直接关联。
    \item 假设七：假设每个分拣中心的货量与日期（时间序列数据）具有一定的函数关系，使得随机森林等机器学习模型可以利用这些历史数据来预测未来的货量。
\end{enumerate}

\clearpage
\section{问题一的建模与求解}
\subsection{分析}
问题一给出了分拣中心在过去的四个月内每天的货量以及过去30天内每个小时的货量。在这些数据的基础上要对下一个月（30天）的货量进行预测，我们先对给出的数据进行观察。我们选取，例如{\tt SC6,SC41} 来观察。利用{\tt python/matplotlib}作图

\begin{figure}[!ht]
	\centering
	\includegraphics*[width=0.8\linewidth]{images/example.pdf}
	\caption{{\tt SC6,SC41}的日期-货量图线}
\end{figure}

根据观察，我们发现在特定的时间段会出现一些"异常值"，而这些异常值的出现很可能是由于节假日等因素引起的。因此为了预测30天后的货物量，我们选取几种深度学习的方法来进行预测。
\begin{enumerate}
	\item LSTM
	\item ARIMA
	\item 随机森林算法
\end{enumerate}
\subsubsection{LSTM}
长短期记忆（英语：Long Short-Term Memory，LSTM）是一种时间循环神经网络（RNN）。由于独特的设计结构，LSTM适合于处理和预测时间序列中间隔和延迟非常长的重要事件。LSTM 通过引入“门”结构来调节信息的流动，这使得它在长序列数据上表现得更好。LSTM的核心组件有遗忘门（Forget Gate）、输入门（Input Gate）、细胞状态（Cell State）、输出门（Output Gate）。LSTM 的每个时刻会通过上述四个门控制信息的流动。遗忘门决定丢弃哪些过去的信息，输入门和它的候选值共同决定将哪些新信息加入细胞状态。细胞状态随后更新，最后通过输出门确定应输出什么信息。\cite{GRAVES2005602}\cite{6795963}

通过这样的结构，LSTM 能够在处理序列数据时有效地保留长期依赖信息，解决了传统RNN在长序列上的梯度消失或爆炸问题。

\subsubsection{ARIMA}
ARIMA模型（自回归积分滑动平均模型）是一种广泛应用于时间序列预测的统计模型。ARIMA 模型结合了自回归（AR）、差分（I）和移动平均（MA）三种主要组成部分，适用于分析和预测具有时间依赖性的数据。其核心组件包括自回归（AR）部分、差分（I）部分、移动平均（MA）部分。将这三部分结合起来，ARIMA模型的完整形式可以表示为：
\begin{align*}
	\phi (B)\nabla^d X_t = \theta(B)a_t
\end{align*}
其中\(\phi (B) X_t = \phi_1X_{t-1}+\phi_2X_{t-2}+\cdots+\phi_p X_{t-p}\)，$p$是自回归项的阶数,\(\phi_i\)是自回归系数，\(B\)是退后算子。\(\theta(B) 	a_t\)则类似，\(a_t\)是时间序列的误差项。\cite{Newbold1974ExperienceWF}

从式子中可以看出，通过对原始数据进行差分转换，ARIMA能够将非平稳时间序列转换为平稳时间序列，因此模型能够适用于预测那些显示出趋势或季节性模式的数据。
\subsubsection{随机森林算法}
随机森林是一种集成学习方法，特别适合用于分类、回归和其他任务，通过构建多棵决策树在训练时并输出模式的类（分类）或平均预测（回归）。随机森林算法的核心思想是通过合并多个决策树的预测结果来提高整体模型的预测准确性和稳定性。\cite{breiman2001random}


随机森林算法的主要步骤有：自助采样（Bootstrap sampling），从原始数据集中随机（允许重复的）选择N个样本构成了一个训练集。N次采样产生的N个训练集被用于训练N棵决策树
；构建决策树：对于每棵树的每个节点，随机选择k个特征（而不是所有特征），然后使用这些特征中的最佳分割方法来分割节点。这种“特征随机选择”的做法增加了树之间的差异性。
决策树的生长：每棵树都尽可能地生长而不进行剪枝。每棵树都完全依赖于自助采样得到的训练数据集来建立。
聚合预测：对于分类问题，使用多数投票法；对于回归问题，计算所有树的输出值的平均值。

随机森林算法的成功在于它的简单性和在多种数据集上表现出的高效性与准确性。它既能处理分类问题，也能处理回归问题，同时还能进行特征选择，这使它成为一种非常强大且灵活的机器学习算法。虽然随机森林通常不如专门的时间序列或图数据模型那样直接适用于处理分拣中心之间的货物流动问题，但它仍然可以在某些情况下提供有价值的见解和预测，特别是随机森林可以提供关于哪些特征（例如历史货物流量、时间因素、分拣中心之间的距离等）对预测货物流量最有影响的洞察。这对于理解货物流量模式和优化物流网络非常有用。


\subsection{按天的数据模拟与预测}
根据选择，我们对其进行数据模拟。我们通过使用{\tt python}的{\tt sklearn}中提供的模型来进行拟合。首先我们通过分类不同的仓库，获取所有的信息。在分类完成之后，我们利用几种方法来实现。首先是利用LSTM来生成预测的曲线。\footnote{具体代码在附录中。}如图\ref{lstm}。

相同的，我们选取随机森林进行预测可以得到图\ref{rf}。另外由于我们发现数据有较大的波动，而 ARIMA 模型适用于平稳时间序列，而从图中看，原始数据不是平稳的。因此直接使用 ARIMA 模型可能不会得到好的预测效果。我们尝试季节性 ARIMA (SARIMA) 模型。

经过所输出的平均方差的对比，我们认为使用随机森林算法对于第一题是相对的最优解。因此我们对未来一个月之内每一个小时的货量预测也采取完全相同的方法预测。



\begin{figure}[t]
	\centering
	\includegraphics*[width=0.8\linewidth]{images/LSTM_R.pdf}
	\caption{利用LSTM预测图线示例}
	\label{lstm}
\end{figure}
\begin{figure}[!ht]
	\centering
	\includegraphics*[width=0.8\linewidth]{images/pre_10.pdf}
	\caption{利用随机森林预测图线示例（SC10）}
	\label{rf}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics*[width=0.8\linewidth]{images/pre_55.pdf}
	\caption{利用SARIMA预测图线示例（SC55）}
	\label{sarima}
\end{figure}

\subsection{按小时的数据模拟和预测}
按照之前的方法，相似的预测之后的数据，如图\ref{SC4_2}。从图中可以看出，拟合效果符合数据波动的周期性。通过计算拟合模型与真实数据之间的平均方差，我们确定使用随机森林算法来拟合数据。

\begin{figure}[!ht]
	\centering
	\includegraphics*[width=0.9\linewidth]{images/SC_4_2.pdf}
	\caption{利用随机森林模型预测小时图线示例（SC4）}
	\label{SC4_2}
\end{figure}

\clearpage
\section{问题二的建模与求解}
\subsection{分析与建模求解}
我们需要对已知的关系进行分析。首先先将仓库之间的关联进行可视化，我们选用弦图的形式，如图\ref{xt}。由于在现有的运输关系模型上发生了变动，于是我们需要使用模型在学习之后改变参数来进行预测。因此我们的流程大概为数据预处理、特征提取、模型训练和预测。
\begin{figure}[!ht]
	\centering
	\includegraphics*[width=0.8\linewidth]{images/xuantu.pdf}
	\caption{分配中心之间的关系弦图}
	\label{xt}
\end{figure}
\subsubsection{数据预处理}
首先我们需要使用独热编码将分类变量转换为数值。独热编码（One-Hot Encoding）是一种处理分类数据的常用方法，由于许多算法更好地处理数值数据而不是文本数据，因此这种方法尤其适用于机器学习模型中。在独热编码中，每个类别值都被转换为一个二进制向量，该向量中只有一个位置是1，其余位置都是0。在这个问题中独热编码被用于“分拣中心”这个分类特征。这样做可以将这些文本标签转换为模型可解释的数值格式，同时保持不同分拣中心之间的独立性。

\subsubsection{时间序列预测}

时间序列预测主要涉及根据过去的数据点预测未来的值，也就是将时间转换为一个可以作为模型输入的特征。在这里我们将日期时间对象转换为相对于起始时间点的总秒数来表示的，这种转换使得模型能够理解时间的线性进展。

\subsubsection{退火算法}
在这里我们也可以选择使用退火算法求解，所谓退火算法（Simulated Annealing，SA）是一种用于寻找大规模搜索空间中的近似全局最优解的随机化算法，它受到物理中固体退火过程的启发。虽然该算法在很多优化问题中表现出了不错的效果，但它也存在一些缺点：


\begin{enumerate}
    \item 收敛速度慢：退火算法的一个主要缺点是它的收敛速度相对较慢，尤其是在接近全局最优解时。算法需要执行大量的迭代和随机搜索步骤，这在大型问题上可能导致较高的计算成本。
    \item 参数依赖性强：退火算法的效果在很大程度上依赖于其参数的设定，如初始温度、冷却速率和温度下降函数等。不恰当的参数设置可能导致算法效果不佳，如过早冷却可能使算法陷入局部最优解。
    \item 随机性：作为一种基于概率的优化方法，退火算法的性能可能因随机性而波动。这意味着同一个问题的多次运行可能会得到不同的结果。
    \item 解的质量保证问题：由于退火算法通常只能找到近似解，而不是确切的全局最优解。
\end{enumerate}
尽管如此，我们依然实现了退火算法的代码。以下是大概的流程：

\begin{enumerate}
    \item 生成初始解：
    随机为每个日期和班次分配200名员工中的员工，确保每名员工都至少被分配到一个班次。
    \item 目标函数的定义：定义一个目标函数来计算解的总成本，包括：\\
    \hspace*{1cm}1）每个员工的总天数成本（如果超过了允许的最大工作天数）。\\
    \hspace*{1cm}2）每个班次中全职和临时员工数目的成本（如果班次的人员配置无法满足需求量）。\\ 
    \hspace*{1cm}3）员工工作日数的方差惩罚，以平衡员工的工作负荷。
    \item 生成邻域解：随机选择一个日期和两个班次，交换这两个班次中的一名员工，以生成新的邻域解。
    \item 模拟退火算法：从初始解开始，通过温度控制和接受概率（根据成本差异和当前温度）逐步更新解，以期找到成本更低的解决方案。
    算法使用了温度衰减因子和迭代次数，直到温度低于最小阈值。
\end{enumerate}

预期的结果是模拟退火算法自动化了员工排班过程，旨在平衡员工的工作时间并尽可能满足每个班次的需求量，同时保持成本效益。然而事实上效果并不如人意，因此我们放弃了这个结果，最后选择使用随机森林算法进行拟合。

\subsubsection{随机森林回归}
接下俩首先将时间（完整时间）转换为自起始时间以来的总秒数，这样可以将问题转换为一个回归问题，其中输入特征是连续的时间值。齐次模型训练中使用上述时间特征和历史货量数据训练随机森林模型。最后将未来的时间点同样转换为自起始时间以来的总秒数，使用训练好的随机森林模型预测这些时间点的货量。

这种方法利用了随机森林的强大能力来处理可能的非线性关系，并可以很好地应对由于特征与目标之间复杂关系造成的预测挑战。我们在随机森林中选取小时，日，周，月作为特征进行训练，最后以模型更改线路连接的参数获取预测的数据。具体代码详见附录。



\clearpage
\section{问题三的建模与求解}

\subsection{分析}

\begin{table}[!ht]
\renewcommand\arraystretch{1.2}
\caption{符号说明}%标题 
\centering%把表居中
\begin{tabular}{p{3cm}<{\centering}p{11cm}<{\centering}}%四个c代表该表一共四列，内容全部居中
\toprule%第一道横线
符号&说明 \\
\midrule%第二道横线 
$R(c,d,s)$&表示第 \(c\)个分拣中心在第 
$d$ 天的第 
$s$ 个班次中正式工的出勤人数。\\
$T(c,d,s)$&表示第 \(c\)个分拣中心在第 
$d$ 天的第 
$s$ 个班次中临时工的出勤人数。\\
$V(c,d,s)$& 表示第 \(c\)个分拣中心在第 
$d$ 天的第 
$s$ 个班次中预测货物量\\
\bottomrule%第三道横线
\end{tabular}
\end{table}
我们的目标是最小化总人天数，即所有分拣中心在所有班次中正式工和临时工的总人数。目标函数可以写为
\begin{align*}
    {\rm Minimize\;}Z = \sum_{c}\sum_{d}\sum_{s}(R(c,d,s)+T(c,d,s))
\end{align*}

约束条件是
\begin{enumerate}
    \item 处理货物量约束：每个班次的员工人数必须足以处理该班次的预测货物量。如果假设一个正式员工每小时可以处理25个单位的货物，一个临时员工每小时可以处理20个单位，则每个班次的约束为：\begin{align*}
        25R(c,d,s)+20 T(c,d,s)\geq V(c,d,s)
    \end{align*}
    \item 正式员工数量限制：每个分拣中心每天的正式员工数量不能超过60人。对于每个日期 
    $d$ 和分拣中心 
    $c$，约束可以写为：\begin{align*}
        \sum_s R(c,d,s)\leq 60
    \end{align*}
\end{enumerate}

\subsection{建模与求解}
针对这个问题，我们选择开源的Python/PuLP库求解。PuLP 作为一个Python库，可以定义问题、变量、目标函数和约束，然后使用 CBC 或其他求解器求解这些问题。这里我们选择在 PuLP 库中调用 CBC（Coin-or branch and cut）解决这个问题。

我们将约束条件和目标函数录入Python脚本中，对每一个分拣中心单独进行运算。如下是运行时截取的一小段结果。可以看出效果较好，能够在可接受的时间内解决问题。（使用容忍度 Gap 为0.01）
\begin{lstlisting}
Result - Optimal solution found (within gap tolerance)

Objective value:                7569.00000000
Lower bound:                    7514.735
Gap:                            0.01
Enumerated nodes:               167447
Total iterations:               849255
Time (CPU seconds):             101.51
Time (Wallclock seconds):       30.31

Option for printingOptions changed from normal to all
Total time (CPU seconds):       101.51   (Wallclock seconds):       30.31
\end{lstlisting}
我们选取一天的安排来查看。这是随机选取的一个分拣中心的预测货量。
\begin{table}[!ht]
\renewcommand\arraystretch{1.2}
\caption{SC14的按小时预测货量}%标题 
\centering%把表居中
\begin{tabular}{p{4cm}<{\centering}p{4cm}<{\centering}p{2cm}<{\centering}p{4cm}<{\centering}}%四个c代表该表一共四列，内容全部居中
\toprule%第一道横线
分配中心&日期&小时&预测货量 \\
\midrule%第二道横线 
SC14&2023-12-01&0&71.81666666666666\\ SC14&2023-12-01&1&71.81666666666666\\ SC14&2023-12-01&2&71.81666666666666\\ SC14&2023-12-01&3&71.81666666666666\\ SC14&2023-12-01&4&71.81666666666666\\ SC14&2023-12-01&5&101.65666666666668\\ SC14&2023-12-01&6&117.33666666666666\\ SC14&2023-12-01&7&68.72666666666667\\ SC14&2023-12-01&8&76.79154761904765\\ SC14&2023-12-01&9&104.46576984126985\\ SC14&2023-12-01&10&106.61277777777778\\ SC14&2023-12-01&11&149.3447380952381\\ SC14&2023-12-01&12&2.7871190476190475\\ SC14&2023-12-01&13&102.0495714285714\\ SC14&2023-12-01&14&168.87423015873014\\ SC14&2023-12-01&15&148.3816865079365\\ SC14&2023-12-01&16&112.3909404761905\\ SC14&2023-12-01&17&99.52757570207568\\ SC14&2023-12-01&18&43.27497619047618\\ SC14&2023-12-01&19&132.01944444444447\\ SC14&2023-12-01&20&115.97620601620604\\ SC14&2023-12-01&21&77.28322222222221\\ SC14&2023-12-01&22&54.38195238095238\\ SC14&2023-12-01&23&17.495702380952384\\
\bottomrule%第三道横线
\end{tabular}
\end{table}

这是安排好的排班表。

\begin{table}[!ht]
\renewcommand\arraystretch{1.2}
\caption{SC14的排班表}%标题 
\centering%把表居中
\begin{tabular}{p{2.5cm}<{\centering}p{2.5cm}<{\centering}p{2.5cm}<{\centering}p{2.5cm}<{\centering}p{2.5cm}<{\centering}}%四个c代表该表一共四列，内容全部居中
\toprule%第一道横线
分配中心&日期&排班时间表&正式员工&临时员工 \\
\midrule%第二道横线 
SC14&2023-12-01&00:00-08:00&26.0&0.0\\
SC14&2023-12-01&05:00-13:00&9.0&11.0\\
SC14&2023-12-01&08:00-16:00&13.0&5.0\\
SC14&2023-12-01&12:00-20:00&2.0&17.0\\
SC14&2023-12-01&14:00-22:00&7.0&1.0\\
SC14&2023-12-01&16:00-24:00&3.0&0.0\\
\bottomrule%第三道横线
\end{tabular}
\end{table}
可以看到满足所需的要求，并且能够按照预测的货物量准确的选取员工。如图所示，条形图部分是正式员工和临时员工处理货物量对排班时间的平均。需要注意的是，在排班间存在重合，在15小时附近可能受到左右四排班的叠加，是可以满足需求的。经过计算检验，可以初步认定效果良好。
\begin{figure}
    \centering
    \includegraphics*[width=0.8\linewidth]{images/pb.pdf}
    \caption{员工的货物处理量与预测货物量示意图}
\end{figure}





\clearpage
\section{问题四的建模与求解}

\subsection{问题四的分析}

这个问题是关于如何调度员工的最优解。这是一个线性规划问题，目标是最小化总人力成本，并同时满足工作班次的需求。下面是关于一些变量的假设。


\begin{table}[!ht]
\renewcommand\arraystretch{1.2}
\caption{符号说明}%标题
\centering%把表居中
\begin{tabular}{p{4cm}<{\centering}p{10cm}<{\centering}}%四个c代表该表一共四列，内容全部居中
\toprule%第一道横线
符号&说明 \\
\midrule%第二道横线 
$F(i,d,s)$&二元变量，表示固定员工 $i$ 在日期 $d $和班次$ s$ 是否工作\\
$T(d,s)$&整数变量，表示在日期 $d $和班次 $s $需要的临时工人数\\
$D(d,s)$&整数变量，表示在日期 $d $和班次 $s $所需的工作需求\\
\bottomrule%第三道横线
\end{tabular}
\end{table}
我们的目标函数是
\begin{align*}
    {\rm Minimize}\quad Z =\sum_{d\in {\rm Dates}}\sum_{s\in {\rm Shifts}}\left(\sum_{i = 1}^{200}F(i,d,s)+T(d,s)\right)
\end{align*}
即最小化全职和临时工的总人天数。这个最小化函数需要满足以下几个约束条件：
\begin{enumerate}
    \item 需求满足：\begin{align*}
        \forall d\in {\rm Dates},\forall s\in {\rm Shifts}:\sum_{i = 1}^{200}F(i,d,s)+20\times T(d,s)\geq D(d,s)
    \end{align*}
    这个约束确保每个班次的工作需求被满足。这里，25和20是每个工作的产出（或能力）。
    \item 正式工出勤率不超过85\%: \begin{align*}
        \forall i:\sum_{d\in {\rm Dates}}\sum_{s\in {\rm Shifts}}F(i,d,s)\leq 30 \times 0.85
    \end{align*}此约束限制每个全职员工在一个月内的最大工作天数。
    \item 连续工作天数不超过7天:\begin{align*}
        \forall i,\forall d\in {\rm Dates\; if \;} d+6 \leq{\rm len(Dates):}\sum_{k =0}^6\sum_{s\in {\rm Shifts}}   F(i,d+k,s)\leq 7
    \end{align*}这确保任何全职员工在连续7天内的工作天数不超过7天。
    \item 每人每天只能工作一个班次：每个员工每天只能安排一个班次\begin{align*}
        \sum_s F(i,d,s)\leq 1
    \end{align*}
\end{enumerate}

\subsection{建模求解}
与问题三相同，我们使用PuLP求解。

由于数量较大，我们设置了一个容忍度\(0.001\)，这是我们在这个容忍度下得到的结果。
\begin{lstlisting}
Result - Optimal solution found (within gap tolerance)

Objective value:                204322.00000000
Lower bound:                    204196.332
Gap:                            0.00
Enumerated nodes:               0
Total iterations:               0
Time (CPU seconds):             1.64
Time (Wallclock seconds):       1.85

Option for printingOptions changed from normal to all
Total time (CPU seconds):       1.72   (Wallclock seconds):       1.94

CSV file has been saved with the scheduling results.
\end{lstlisting}

可以看出运算结果符合预期。


\clearpage
\section{总结}
\subsection{使用随机森林的优点}
\begin{enumerate}
    \item 随机森林算法（RF）不太容易过拟合，因为 RF 本质上是模型集成（model ensemble），从 Leo Breiman 的理论来看 RF 也不会因为 树 数量的增加，而导致过拟合，因为这些数都是集合在一起的单独模型，效果不好的树会被 降低权重（downvote）。\cite{2001Random}
    \item 数据分布要求低。RF 不像线性模型，不要求数据分布符合正态分布，来得到统计结果上的近似。因此任意的数据分布都可以使用 RF。
    \item 特征工程：对于一些简单的线性模型，为了增加特征，我们往往需要增加如$ x_1^2 + x_2^2 $等特征来作为模型的输入，帮助模型构建更多的特征。但是在 RF 中，这些基础的特征工程是不必要的。但是在本题的求解过程中，我们使用了高阶特征工程，以此增加 RF 的精度（类似日期上的处理，提取出月份，周数等）。
    \item 数据预处理：类似神经网络需要对数据作预处理来得到 0 ～ 1 之间的数据分布，在 RF 这里往往都不太需要，因此 RF 对于数据的要求也不高。
    \item 求解速度快，求解准确性好（mse最小）
\end{enumerate}
\subsection{问题一和问题二建模存在的缺点}
由于在统计过程中可以发现由于出现了一些异常值，这些异常值体现为节假日或者特殊情况。我们如果可以实现多模型集体学习，或许可以将各种模型的优点结合起来。同时，使用随机森林算法在本问题的求解过程中有以下不足：RF 的范化能力较强，不太容易出现对极值的反应。当然我们也因此，在模型的求解过程中，我们添加了“2023-12-01 至 2023-12-31 期间不会发生如购物节等可能导致货物量激增的特殊事件”这一基本假设。



\subsection{PULP效果评估}

使用 PULP（Python用于优化）模型来解决排班问题具有优点：
\begin{enumerate}
    \item 灵活性:PULP 模型能够处理各种约束条件和目标函数，因此适用于各种不同的排班场景。在这个情景中，我们能够轻松地添加和修改约束条件和目标函数来满足特定的需求。
    \item 求解器支持:PULP 支持多种优化求解器，包括 CBC、GLPK、GUROBI 等。这些求解器能够高效地解决中等规模的优化问题。
\end{enumerate}


当然也存在以下缺点：
\begin{enumerate}
    \item 性能受限:PULP 适用于中等规模的优化问题，对于大规模的问题可能存在性能上的限制。当问题规模较大时，求解可能会变得非常耗时。
    \item 精确度问题:由于某些算法的限制，PULP 可能不能总是找到全局最优解，尤其是对于复杂的非线性问题。在实践中，可能需要通过调整求解器参数或者使用其他方法来提高精确度。
    \item 难以处理复杂约束:虽然 PULP 提供了丰富的约束类型和灵活的约束表达方式，但对于某些复杂的约束条件，可能需要额外的技巧或者结合其他工具来处理。
\end{enumerate}



结合情景分析：
在这个排班问题的情景下，使用 PULP 是合适的选择。该问题的约束条件相对简单，且规模适中，不会导致求解器性能方面的问题。通过 PULP，我们能够轻松地建立模型，定义约束条件和目标函数，并使用求解器求解得到最优的排班方案。然而，如果问题规模变得非常大，例如涉及数千名员工和数十个班次的情况，那么可能需要考虑更高效的方法来解决。

\clearpage


\bibliographystyle{plain}
\bibliography{ref}



\clearpage
\begin{center}
    \huge \bf 附录
\end{center}
\appendix
\section{问题一代码}
\noindent 随机森林调参代码
\begin{lstlisting}[language=python]
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

# 假设存在的SCid列表
data_for_sc = pd.read_csv("../../附件/附件1.csv", encoding="GB2312")
ALL_SC = list(set(data_for_sc["分拣中心"]))
existing_scs = list(map(lambda SC_: int(SC_[2:]), ALL_SC))
existing_scs.sort()


# 加载数据
def load_data(existing_scs):
    all_data = []
    for sc_id in existing_scs:
        try:
            data = pd.read_csv(f"SC{sc_id}.csv")
            data["center_id"] = sc_id
            all_data.append(data)
        except FileNotFoundError:
            print(f"File for center {sc_id} not found.")
    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()


# 数据清洗和预处理
def preprocess_data(data):
    data["date"] = pd.to_datetime(data["date"], errors="coerce")
    data.dropna(subset=["date", "value"], inplace=True)

    data["year"] = data["date"].dt.year
    data["month"] = data["date"].dt.month
    data["day"] = data["date"].dt.day
    data["weekday"] = data["date"].dt.weekday

    scaler = StandardScaler()
    data[["year", "month", "day", "weekday"]] = scaler.fit_transform(
        data[["year", "month", "day", "weekday"]]
    )

    return data


# 模型训练和参数调整
def train_and_optimize_model(X_train, y_train):
    param_grid = {
        "n_estimators": [100 * i for i in range(1, 10)],
        "max_depth": [10 * i for i in range(1, 10)],
        "min_samples_split": [10 * i for i in range(1, 10)],
    }
    model = RandomForestRegressor(random_state=42)
    grid_search = GridSearchCV(
        model, param_grid, cv=5, scoring="neg_mean_squared_error", verbose=2
    )
    grid_search.fit(X_train, y_train)

    print("Best parameters:", grid_search.best_params_)
    return grid_search.best_estimator_


if __name__ == "__main__":
    data = load_data(existing_scs)
    if not data.empty:
        data = preprocess_data(data)
        features = data[["center_id", "year", "month", "day", "weekday"]]
        target = data["value"]

        X_train, X_test, y_train, y_test = train_test_split(
            features, target, test_size=0.2, random_state=42
        )

        best_model = train_and_optimize_model(X_train, y_train)

        y_pred = best_model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        print(f"Test MSE: {mse}")
    else:
        print("No data loaded, please check the data files.")
\end{lstlisting}

\noindent 随机森林训练代码
\begin{lstlisting}[language=python]
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 假设数据已经按照分拣中心编号和日期排列好
# 每个文件名格式为 'SC{i}.csv'，其中 i 是分拣中心编号
# 假设存在的SCid列表
data_for_sc = pd.read_csv("../../附件/附件1.csv", encoding="GB2312")
ALL_SC = list(set(data_for_sc["分拣中心"]))
existing_scs = list(map(lambda SC_: int(SC_[2:]), ALL_SC))
existing_scs.sort()

# 加载数据
def load_data(existing_scs):
    all_data = []
    for i in existing_scs:
        data = pd.read_csv(f"SC{i}.csv")
        data["SC_id"] = i
        all_data.append(data)
    return pd.concat(all_data, ignore_index=True)


# 数据预处理
def preprocess(data):
    # 假设数据包含日期和货量两列，日期列名为 'date'，货量列名为 'value'
    data["date"] = pd.to_datetime(data["date"])
    data["year"] = (pd.to_datetime(data["date"])).dt.year
    data["month"] = (pd.to_datetime(data["date"])).dt.month
    data["day"] = (pd.to_datetime(data["date"])).dt.day
    data["weekday"] = (pd.to_datetime(data["date"])).dt.weekday
    return data


# 训练模型
def train_model(data):
    features = data[["SC_id", "year", "month", "day", "weekday"]]
    target = data["value"]
    X_train, X_test, y_train, y_test = train_test_split(
        features,
        target,
        test_size=0.2,
        random_state=50,

    )

    model = RandomForestRegressor(n_estimators=300, random_state=42, min_samples_split=20)
    model.fit(X_train, y_train)

    # 预测和评估
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    print(f"Mean Squared Error: {mse}")
    return model


# 预测未来的货量
def predict_future(model, start_date, num_days, existing_scs):
    future_dates = pd.date_range(start_date, periods=num_days)
    future_data = pd.DataFrame(
        {
            "date": np.repeat(future_dates, len(existing_scs)),
            "SC_id": np.tile(existing_scs, num_days),
        }
    )
    future_data = preprocess(future_data)
    features = future_data[["SC_id", "year", "month", "day", "weekday"]]
    predictions = model.predict(features)
    future_data["predicted_volume"] = predictions
    return future_data


# 保存结果到CSV
def save_predictions_to_csv(predictions, file_name):
    predictions["date"] = predictions["date"].dt.strftime("%Y/%m/%d")
    predictions.to_csv(file_name, index=False)
    print(f"Saved predictions to {file_name}")


# 主函数
def main():
    data = load_data(existing_scs)
    data = preprocess(data)
    model = train_model(data)
    future_predictions = predict_future(model, "2023-08-01", 153, existing_scs)
    save_predictions_to_csv(future_predictions, "predicted_volumes.csv")


if __name__ == "__main__":
    main()
\end{lstlisting}
ARIMA和LSTM的代码未被采用，因此仅放在支撑材料中。


\section{问题二代码}
\noindent 按小时预测
\begin{lstlisting}[language=python]
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder


def load_data():
    # 修改这里以加载附件2
    data_sc = pd.read_csv("附件2.csv", encoding="gb2312")
    data_routes = pd.read_csv("附件3.csv", encoding="gb2312")
    data_changes = pd.read_csv("附件4.csv", encoding="gb2312")
    return data_sc, data_routes, data_changes



def preprocess_data(data_sc, data_changes):
    # Combining date and hour into a single datetime column
    data_sc["完整时间"] = pd.to_datetime(
        data_sc["日期"].astype(str) + " " + data_sc["小时"].astype(str) + ":00"
    )

    # Adding new time-related features
    data_sc["小时"] = data_sc["完整时间"].dt.hour  # Hour of the day
    data_sc["星期几"] = data_sc["完整时间"].dt.weekday  # Day of the week (Monday=0, Sunday=6)
    data_sc["月份"] = data_sc["完整时间"].dt.month  # Month of the year

    # One-Hot Encoding for sorting centers
    ohe = OneHotEncoder()
    encoded_centers = ohe.fit_transform(data_sc[["分拣中心"]].astype(str)).toarray()
    center_df = pd.DataFrame(encoded_centers, columns=ohe.get_feature_names_out())
    data_sc = pd.concat([data_sc, center_df], axis=1)

    return data_sc, ohe


def predict_future_volume(data_sc, ohe, data_changes):
    # Generating data for the future date range within each hour
    last_time = data_sc["完整时间"].max()
    future_dates = pd.date_range(last_time + pd.Timedelta(hours=1), periods=744, freq="h")
    future_data = pd.DataFrame({"完整时间": future_dates})
    future_data["小时"] = future_data["完整时间"].dt.hour
    future_data["星期几"] = future_data["完整时间"].dt.weekday
    future_data["月份"] = future_data["完整时间"].dt.month

    # Predicting future volume for each sorting center
    future_volumes = []
    centers = ohe.get_feature_names_out()
    for center in centers:
        center_data = data_sc[data_sc[center] == 1]
        if not center_data.empty:
            # Train a model to predict volume
            X = center_data[["小时", "星期几", "月份"]].values
            y = center_data["货量"].values
            model = RandomForestRegressor(n_estimators=100, random_state=42)
            model.fit(X, y)

            # Use the model to predict future volume
            future_X = future_data[["小时", "星期几", "月份"]].values
            future_y = model.predict(future_X)
            for date, volume in zip(future_dates, future_y):
                cleaned_center_name = center.replace("x0_分拣中心_", "")
                future_volumes.append(
                    [cleaned_center_name, date.strftime("%Y/%m/%d %H:%M"), volume]
                )

    future_volumes_df = pd.DataFrame(
        future_volumes, columns=["分拣中心", "日期时间", "货量"]
    )

    # Considering changes in routing paths from Attachment 4
    future_volumes_df.set_index(["分拣中心", "日期时间"], inplace=True)
    for index, row in data_changes.iterrows():
        from_center = "x0_分拣中心_" + row["始发分拣中心"]
        to_center = "分拣中心_" + row["到达分拣中心"]
        if from_center in centers:
            future_volumes_df.loc[(to_center,), "货量"] += (
                future_volumes_df.loc[(from_center,), "货量"] * 0.1
            )
            future_volumes_df.loc[(from_center,), "货量"] *= 0.9

    future_volumes_df.reset_index(inplace=True)
    return future_volumes_df


def main():
    data_sc, data_routes, data_changes = load_data()
    data_sc, ohe = preprocess_data(data_sc, data_changes)
    future_volumes_df = predict_future_volume(data_sc, ohe, data_changes)
    future_volumes_df.to_csv("predicted_future_volumes_hours.csv", index=False)
    print("预测完成，结果已保存到 'predicted_future_volumes_hours.csv'")


if __name__ == "__main__":
    main()
\end{lstlisting}
\noindent 按天预测
\begin{lstlisting}[language=python]
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error


def load_data():
    # 加载数据
    data_sc = pd.read_csv("附件1.csv", encoding="gb2312")
    data_routes = pd.read_csv("附件3.csv", encoding="gb2312")
    data_changes = pd.read_csv("附件4.csv", encoding="gb2312")
    return data_sc, data_routes, data_changes


def preprocess_data(data_sc, data_changes):
    # Convert '日期' to datetime and extract more features
    data_sc["日期"] = pd.to_datetime(data_sc["日期"], format="%Y/%m/%d")
    data_sc["月份"] = data_sc["日期"].dt.month  # Month as a feature
    data_sc["星期几"] = data_sc["日期"].dt.weekday  # Day of the week as a feature
    data_sc["年中日"] = data_sc["日期"].dt.dayofyear  # Day of the year as a feature

    # One-Hot Encoding for sorting centers
    ohe = OneHotEncoder()
    encoded_centers = ohe.fit_transform(data_sc[["分拣中心"]].astype(str)).toarray()
    center_df = pd.DataFrame(encoded_centers, columns=ohe.get_feature_names_out())
    data_sc = pd.concat([data_sc, center_df], axis=1)

    return data_sc, ohe


def predict_future_volume(data_sc, ohe, data_changes):
    # Generate future dates starting 120 days after the minimum date in the data
    future_dates = pd.date_range(start=data_sc["日期"].max() + pd.Timedelta(days=1), periods=31, freq="D")
    future_data = pd.DataFrame({"日期": future_dates})
    future_data["月份"] = future_data["日期"].dt.month
    future_data["星期几"] = future_data["日期"].dt.weekday
    future_data["年中日"] = future_data["日期"].dt.dayofyear

    # Predict future volume for each sorting center
    future_volumes = []
    centers = ohe.get_feature_names_out()
    for center in centers:
        center_data = data_sc[data_sc[center] == 1]
        if not center_data.empty:
            # Train a model to predict volume
            X = center_data[["年中日", "月份", "星期几"]]
            y = center_data["货量"]
            model = RandomForestRegressor(n_estimators=100, random_state=42)
            model.fit(X, y)

            # Predict future volume
            future_X = future_data[["年中日", "月份", "星期几"]]
            future_y = model.predict(future_X)
            for date, volume in zip(future_dates, future_y):
                cleaned_center_name = center.replace("x0_分拣中心_", "")
                future_volumes.append(
                    [cleaned_center_name, date.strftime("%Y/%m/%d"), volume]
                )

    future_volumes_df = pd.DataFrame(
        future_volumes, columns=["分拣中心", "日期", "货量"]
    )

    # Adjust predicted volumes based on changes from Attachment 4
    future_volumes_df.set_index(["分拣中心", "日期"], inplace=True)
    for index, row in data_changes.iterrows():
        from_center = "x0_分拣中心_" + row["始发分拣中心"]
        to_center = "分拣中心_" + row["到达分拣中心"]
        if from_center in centers:
            future_volumes_df.loc[(to_center,), "货量"] += (
                future_volumes_df.loc[(from_center,), "货量"] * 0.1
            )
            future_volumes_df.loc[(from_center,), "货量"] *= 0.9

    future_volumes_df.reset_index(inplace=True)
    return future_volumes_df


def main():
    data_sc, data_routes, data_changes = load_data()
    data_sc, ohe = preprocess_data(data_sc, data_changes)
    future_volumes_df = predict_future_volume(data_sc, ohe, data_changes)
    future_volumes_df.to_csv("predicted_future_volumes.csv", index=False)
    print("预测完成，结果已保存到 'predicted_future_volumes.csv'")


if __name__ == "__main__":
    main()
\end{lstlisting}

\section{问题三代码}
\begin{lstlisting}[language=python]
import pandas as pd

# Load the provided CSV file to examine its structure
file_path = "../problem2/predicted_future_volumes_hours.csv"
predicted_volumes = pd.read_csv(file_path)
# Convert the date-time string to a datetime object
predicted_volumes["日期时间"] = pd.to_datetime(predicted_volumes["日期时间"])

# Define the shift times
shift_times = {
    "00:00-08:00": ("00:00", "08:00"),
    "05:00-13:00": ("05:00", "13:00"),
    "08:00-16:00": ("08:00", "16:00"),
    "12:00-20:00": ("12:00", "20:00"),
    "14:00-22:00": ("14:00", "22:00"),
    "16:00-24:00": ("16:00", "24:00"),
}


# Create a function to assign each hour to a shift
def assign_shift(hour):
    for shift, (start, end) in shift_times.items():
        if start <= hour.strftime("%H:%M") < end:
            return shift
    return None


# Apply the function to assign shifts
predicted_volumes["班次"] = predicted_volumes["日期时间"].apply(
    lambda x: assign_shift(x)
)

# Group by center, date, and shift to sum up volumes
grouped_volumes = (
    predicted_volumes.groupby(
        ["分拣中心", predicted_volumes["日期时间"].dt.date, "班次"]
    )["货量"]
    .sum()
    .reset_index()
)
grouped_volumes.rename(columns={"日期时间": "日期"}, inplace=True)

for special_SC in grouped_volumes["分拣中心"].unique():
    pre_grouped_volumes = grouped_volumes[grouped_volumes["分拣中心"] == special_SC]

    import pulp
    from pulp import PULP_CBC_CMD

    # Set up the linear programming problem to minimize the total number of person-days
    model = pulp.LpProblem("Staff_Scheduling", pulp.LpMinimize)

    # Decision variables
    # Number of regular and temporary workers per shift, per day, per sorting center
    regular_workers = pulp.LpVariable.dicts(
        "Regular",
        [
            (center, date, shift)
            for center, date, shift in zip(
                pre_grouped_volumes["分拣中心"],
                pre_grouped_volumes["日期"],
                pre_grouped_volumes["班次"],
            )
        ],
        lowBound=0,
        cat="Integer",
    )
    temporary_workers = pulp.LpVariable.dicts(
        "Temporary",
        [
            (center, date, shift)
            for center, date, shift in zip(
                pre_grouped_volumes["分拣中心"],
                pre_grouped_volumes["日期"],
                pre_grouped_volumes["班次"],
            )
        ],
        lowBound=0,
        cat="Integer",
    )

    # Objective: Minimize the total person-days
    model += pulp.lpSum(
        [
            regular_workers[center, date, shift]
            + temporary_workers[center, date, shift]
            for center, date, shift in zip(
                pre_grouped_volumes["分拣中心"],
                pre_grouped_volumes["日期"],
                pre_grouped_volumes["班次"],
            )
        ]
    )

    # Constraints
    # Each shift's staffing needs must meet the volume requirements
    for i, row in pre_grouped_volumes.iterrows():
        center, date, shift, volume = (
            row["分拣中心"],
            row["日期"],
            row["班次"],
            row["货量"],
        )
        model += (
            25 * regular_workers[center, date, shift]
            + 20 * temporary_workers[center, date, shift]
            >= volume
        )

    # Maximum of 60 regular workers per sorting center per day
    for (center, date), group in pre_grouped_volumes.groupby(["分拣中心", "日期"]):
        model += (
            pulp.lpSum(regular_workers[center, date, shift] for shift in group["班次"])
            <= 60
        )

    # Solve the model
    model.solve(PULP_CBC_CMD(msg=1, threads=8, gapRel=0.05))
    # model.solve()

    # Output results
    results = []
    for center, date, shift in zip(
        pre_grouped_volumes["分拣中心"], pre_grouped_volumes["日期"], pre_grouped_volumes["班次"]
    ):
        result = {
            "分拣中心": center,
            "日期": date,
            "班次": shift,
            "正式工": regular_workers[center, date, shift].varValue,
            "临时工": temporary_workers[center, date, shift].varValue,
        }
        results.append(result)

    results_df = pd.DataFrame(results)
    results_df.to_csv(f"sol3_/{special_SC}scheduling_results.csv", index=False)
\end{lstlisting}

\section{问题四代码}
\begin{lstlisting}[language=python]
import pandas as pd
from pulp import LpProblem, LpMinimize, LpVariable, lpSum, LpBinary, PULP_CBC_CMD

# 读取数据
df = pd.read_csv("for_read.csv")
df["date"] = pd.to_datetime(df["date"]).dt.date

# 班次及时间段
shifts = [(0, 8), (5, 13), (8, 16), (12, 20), (14, 22), (16, 24)]
shift_labels = ["Shift1", "Shift2", "Shift3", "Shift4", "Shift5", "Shift6"]
dates = sorted(df["date"].unique())

# 计算每个班次的需求
demand_per_shift = {date: {} for date in dates}
for date in dates:
    daily_data = df[df["date"] == date]
    for label, (start, end) in zip(shift_labels, shifts):
        demand_per_shift[date][label] = daily_data[
            (daily_data["hour"] >= start) & (daily_data["hour"] < end)
        ]["value"].sum()

# 建立优化模型
model = LpProblem("Personnel_Scheduling", LpMinimize)

# 定义变量
full_time = LpVariable.dicts(
    "FullTime", (dates, shift_labels, range(200)), 0, 1, LpBinary
)
temp_workers = LpVariable.dicts(
    "TempWorkers", (dates, shift_labels), 0, None, cat="Integer"
)

# 目标函数：最小化总人天数
model += lpSum(
    full_time[date][shift][i]
    for date in dates
    for shift in shift_labels
    for i in range(200)
) + lpSum(temp_workers[date][shift] for date in dates for shift in shift_labels)

# 每个班次的需求必须被满足的约束
for date in dates:
    for shift in shift_labels:
        model += (
            25 * lpSum(full_time[date][shift][i] for i in range(200))
            + 20 * temp_workers[date][shift]
            >= demand_per_shift[date][shift]
        )

# 正式工的出勤率不超过85%
for i in range(200):
    model += (
        lpSum(full_time[date][shift][i] for date in dates for shift in shift_labels)
        <= 30 * 0.85
    )

# 正式工连续出勤天数不超过7天
for i in range(200):
    for d in range(len(dates) - 6):
        model += (
            lpSum(
                full_time[dates[d + k]][shift][i]
                for k in range(7)
                for shift in shift_labels
            )
            <= 7
        )

# 每个人每天只能上一个班次的约束
for date in dates:
    for i in range(200):
        model += (
            lpSum(full_time[date][shift][i] for shift in shift_labels) <= 1
        )

# 求解问题
model.solve(PULP_CBC_CMD(msg=1, threads=8, gapRel=0.001))

# 收集结果并输出为CSV
results = []
for date in dates:
    for shift in shift_labels:
        for i in range(200):
            if full_time[date][shift][i].varValue > 0:
                results.append(
                    {
                        "Sorting_Center": "SC60",
                        "Date": date,
                        "Shift": shift,
                        "Employee": f"FullTime({i})",
                    }
                )
        temp_workers_count = int(temp_workers[date][shift].varValue)
        for j in range(temp_workers_count):
            results.append(
                {
                    "Sorting_Center": "SC60",
                    "Date": date,
                    "Shift": shift,
                    "Employee": f"Temp({j})",
                }
            )

# 创建DataFrame并保存到CSV
results_df = pd.DataFrame(results)
results_df.to_csv("scheduling_results_2.csv", index=False)
print("CSV file has been saved with the scheduling results 2.")
\end{lstlisting}


\end{document}