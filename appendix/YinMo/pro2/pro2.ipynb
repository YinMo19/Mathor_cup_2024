{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载历史货物量数据\n",
    "historical_volumes = pd.read_csv(\"附件1.csv\", encoding=\"gb2312\")\n",
    "\n",
    "# 加载历史关联数据\n",
    "historical_links = pd.read_csv(\"附件3.csv\", encoding=\"gb2312\")\n",
    "\n",
    "# 加载未来的分拣中心关联数据\n",
    "future_links = pd.read_csv(\"附件4.csv\", encoding=\"gb2312\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 查看历史货物量数据\n",
    "# print(historical_volumes.head())\n",
    "# print(historical_volumes.info())\n",
    "\n",
    "# # 查看历史关联数据\n",
    "# print(historical_links.head())\n",
    "# print(historical_links.info())\n",
    "\n",
    "# # 查看未来关联数据\n",
    "# print(future_links.head())\n",
    "# print(future_links.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分拣中心    0\n",
      "日期      0\n",
      "货量      0\n",
      "dtype: int64\n",
      "始发分拣中心    0\n",
      "到达分拣中心    0\n",
      "货量        0\n",
      "dtype: int64\n",
      "始发分拣中心    0\n",
      "到达分拣中心    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 转换日期格式\n",
    "historical_volumes['日期'] = pd.to_datetime(historical_volumes['日期'], format='%Y/%m/%d')\n",
    "\n",
    "# 检查是否有缺失值，并考虑填充或删除\n",
    "print(historical_volumes.isnull().sum())\n",
    "print(historical_links.isnull().sum())\n",
    "print(future_links.isnull().sum())\n",
    "\n",
    "# 如果有必要，填充缺失值，这里假设没有缺失值\n",
    "# historical_volumes.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>分拣中心</th>\n",
       "      <th>日期</th>\n",
       "      <th>货量</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SC48</td>\n",
       "      <td>2023-09-05</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SC48</td>\n",
       "      <td>2023-10-24</td>\n",
       "      <td>2092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SC48</td>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SC48</td>\n",
       "      <td>2023-11-16</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SC48</td>\n",
       "      <td>2023-11-09</td>\n",
       "      <td>771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>SC48</td>\n",
       "      <td>2023-09-25</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>SC48</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>SC48</td>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>SC48</td>\n",
       "      <td>2023-09-04</td>\n",
       "      <td>807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>SC48</td>\n",
       "      <td>2023-11-19</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     分拣中心         日期    货量\n",
       "0    SC48 2023-09-05   723\n",
       "1    SC48 2023-10-24  2092\n",
       "2    SC48 2023-08-05   754\n",
       "3    SC48 2023-11-16   754\n",
       "4    SC48 2023-11-09   771\n",
       "..    ...        ...   ...\n",
       "117  SC48 2023-09-25   785\n",
       "118  SC48 2023-08-01   685\n",
       "119  SC48 2023-08-18   710\n",
       "120  SC48 2023-09-04   807\n",
       "121  SC48 2023-11-19   601\n",
       "\n",
       "[122 rows x 3 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择一个分拣中心的数据进行演示\n",
    "sc_data = historical_volumes[historical_volumes['分拣中心'] == 'SC48']\n",
    "sc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06040503],\n",
       "       [0.53840782],\n",
       "       [0.07122905],\n",
       "       [0.07122905],\n",
       "       [0.0771648 ],\n",
       "       [0.04958101],\n",
       "       [0.09392458],\n",
       "       [0.03247207],\n",
       "       [0.01396648],\n",
       "       [0.03421788],\n",
       "       [0.04643855],\n",
       "       [0.96473464],\n",
       "       [0.04993017],\n",
       "       [0.05726257],\n",
       "       [0.04958101],\n",
       "       [0.10265363],\n",
       "       [0.08170391],\n",
       "       [0.05167598],\n",
       "       [0.09951117],\n",
       "       [0.02199721],\n",
       "       [0.07856145],\n",
       "       [0.04888268],\n",
       "       [0.05132682],\n",
       "       [1.        ],\n",
       "       [0.08065642],\n",
       "       [0.04643855],\n",
       "       [0.07367318],\n",
       "       [0.05377095],\n",
       "       [0.04888268],\n",
       "       [0.2224162 ],\n",
       "       [0.05796089],\n",
       "       [0.05761173],\n",
       "       [0.03037709],\n",
       "       [0.06110335],\n",
       "       [0.04888268],\n",
       "       [0.04294693],\n",
       "       [0.05377095],\n",
       "       [0.06145251],\n",
       "       [0.0652933 ],\n",
       "       [0.0247905 ],\n",
       "       [0.05377095],\n",
       "       [0.09706704],\n",
       "       [0.13617318],\n",
       "       [0.06738827],\n",
       "       [0.09567039],\n",
       "       [0.0652933 ],\n",
       "       [0.04958101],\n",
       "       [0.07087989],\n",
       "       [0.08240223],\n",
       "       [0.06599162],\n",
       "       [0.08589385],\n",
       "       [0.04329609],\n",
       "       [0.05097765],\n",
       "       [0.03351955],\n",
       "       [0.0827514 ],\n",
       "       [0.03456704],\n",
       "       [0.06284916],\n",
       "       [0.05761173],\n",
       "       [0.05691341],\n",
       "       [0.03805866],\n",
       "       [0.05761173],\n",
       "       [0.06284916],\n",
       "       [0.06564246],\n",
       "       [0.01152235],\n",
       "       [0.05796089],\n",
       "       [0.07821229],\n",
       "       [0.06424581],\n",
       "       [0.06319832],\n",
       "       [0.15363128],\n",
       "       [0.01606145],\n",
       "       [0.02583799],\n",
       "       [0.08659218],\n",
       "       [0.10195531],\n",
       "       [0.03980447],\n",
       "       [0.03736034],\n",
       "       [0.05377095],\n",
       "       [0.07018156],\n",
       "       [0.04364525],\n",
       "       [0.09567039],\n",
       "       [0.0377095 ],\n",
       "       [0.07541899],\n",
       "       [0.05481844],\n",
       "       [0.0551676 ],\n",
       "       [0.15083799],\n",
       "       [0.08554469],\n",
       "       [0.09427374],\n",
       "       [0.06005587],\n",
       "       [0.05656425],\n",
       "       [0.01710894],\n",
       "       [0.02723464],\n",
       "       [0.12674581],\n",
       "       [0.09357542],\n",
       "       [0.05481844],\n",
       "       [0.42807263],\n",
       "       [0.04923184],\n",
       "       [0.06913408],\n",
       "       [0.03526536],\n",
       "       [0.04748603],\n",
       "       [0.06075419],\n",
       "       [0.12081006],\n",
       "       [0.05446927],\n",
       "       [0.09113128],\n",
       "       [0.15747207],\n",
       "       [0.05900838],\n",
       "       [0.        ],\n",
       "       [0.01326816],\n",
       "       [0.04015363],\n",
       "       [0.0247905 ],\n",
       "       [0.08030726],\n",
       "       [0.07646648],\n",
       "       [0.06668994],\n",
       "       [0.03945531],\n",
       "       [0.15886872],\n",
       "       [0.05446927],\n",
       "       [0.01641061],\n",
       "       [0.10370112],\n",
       "       [0.07122905],\n",
       "       [0.08205307],\n",
       "       [0.04713687],\n",
       "       [0.05586592],\n",
       "       [0.08973464],\n",
       "       [0.01780726]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据标准化\n",
    "scaler = MinMaxScaler()\n",
    "sc_data_scaled = scaler.fit_transform(sc_data[\"货量\"].values.reshape(-1, 1))\n",
    "\n",
    "sc_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建时间序列数据集\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        a = data[i : (i + time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# 使用所有分拣中心的数据\n",
    "all_data_scaled = scaler.fit_transform(historical_volumes[\"货量\"].values.reshape(-1, 1))\n",
    "\n",
    "time_step = 3\n",
    "X, y = create_dataset(all_data_scaled, time_step)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinmo19/Mathor_cup_2024/venv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0153 - val_loss: 0.0028\n",
      "Epoch 2/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 3/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 4/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0032 - val_loss: 0.0027\n",
      "Epoch 5/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 6/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0031 - val_loss: 0.0027\n",
      "Epoch 7/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0031 - val_loss: 0.0025\n",
      "Epoch 8/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 9/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 10/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0033 - val_loss: 0.0024\n",
      "Epoch 11/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 12/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0026 - val_loss: 0.0024\n",
      "Epoch 13/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 14/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0027 - val_loss: 0.0023\n",
      "Epoch 15/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 16/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 17/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0027 - val_loss: 0.0023\n",
      "Epoch 18/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 19/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 20/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0032 - val_loss: 0.0023\n",
      "Epoch 21/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0027 - val_loss: 0.0024\n",
      "Epoch 22/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 23/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 24/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 25/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0029 - val_loss: 0.0024\n",
      "Epoch 26/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0027 - val_loss: 0.0023\n",
      "Epoch 27/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 28/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0030 - val_loss: 0.0024\n",
      "Epoch 29/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 30/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 31/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 32/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 33/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 34/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 35/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 36/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0027 - val_loss: 0.0022\n",
      "Epoch 37/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0027 - val_loss: 0.0023\n",
      "Epoch 38/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 39/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 40/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0027 - val_loss: 0.0022\n",
      "Epoch 41/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 42/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 43/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 44/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0030 - val_loss: 0.0022\n",
      "Epoch 45/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0031 - val_loss: 0.0022\n",
      "Epoch 46/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0027 - val_loss: 0.0022\n",
      "Epoch 47/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 48/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0027 - val_loss: 0.0023\n",
      "Epoch 49/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 50/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 51/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 52/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 53/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 54/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0027 - val_loss: 0.0022\n",
      "Epoch 55/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 56/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 57/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0027 - val_loss: 0.0021\n",
      "Epoch 58/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 59/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 60/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 61/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0027 - val_loss: 0.0023\n",
      "Epoch 62/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 63/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 64/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 65/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 66/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 67/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 68/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 69/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 70/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 71/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 72/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 73/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0027 - val_loss: 0.0022\n",
      "Epoch 74/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 75/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0027 - val_loss: 0.0022\n",
      "Epoch 76/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 77/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 78/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 79/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 80/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 81/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 82/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 83/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 84/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 85/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 86/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 87/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 88/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 89/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 90/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 91/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 92/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0029 - val_loss: 0.0021\n",
      "Epoch 93/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 94/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 95/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 96/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 97/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 98/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 99/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0024 - val_loss: 0.0020\n",
      "Epoch 100/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0022 - val_loss: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, verbose=1)\n",
    "\n",
    "model.save('sc48_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch_geometric.utils import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 historical_links 已经加载\n",
    "# 转换数据为 networkx 图\n",
    "G = nx.from_pandas_edgelist(\n",
    "    historical_links,\n",
    "    source=\"始发分拣中心\",\n",
    "    target=\"到达分拣中心\",\n",
    "    edge_attr=\"货量\",\n",
    "    create_using=nx.DiGraph(),\n",
    ")\n",
    "\n",
    "# 为了能够使用 GCN，我们需要一个无向图\n",
    "G = G.to_undirected()\n",
    "\n",
    "# 将 networkx 图转换为 torch_geometric 数据\n",
    "# 首先为每个节点赋予初始特征（可以是度数等）\n",
    "for node in G.nodes():\n",
    "    G.nodes[node][\"x\"] = [float(G.degree[node])]\n",
    "\n",
    "data = from_networkx(G)\n",
    "data.edge_attr = torch.tensor(\n",
    "    list(nx.get_edge_attributes(G, \"货量\").values()), dtype=torch.float\n",
    ").unsqueeze(1)\n",
    "\n",
    "# 标准化边属性\n",
    "scaler = MinMaxScaler()\n",
    "data.edge_attr = torch.tensor(scaler.fit_transform(data.edge_attr), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, 1)  # Assuming each edge will have one output feature: the cargo volume\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        return self.conv3(x, edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GCN(num_node_features=1, hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 加载数据\n",
    "historical_volumes = pd.read_csv(\"附件1.csv\", encoding=\"gb2312\")\n",
    "historical_links = pd.read_csv(\"附件3.csv\", encoding=\"gb2312\")\n",
    "future_links = pd.read_csv(\"附件4.csv\", encoding=\"gb2312\")\n",
    "\n",
    "# 创建节点列表\n",
    "all_nodes = pd.unique(\n",
    "    historical_links[[\"始发分拣中心\", \"到达分拣中心\"]].values.ravel(\"K\")\n",
    ")\n",
    "node_indices = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "\n",
    "# 创建边索引\n",
    "edge_index = torch.tensor(\n",
    "    [\n",
    "        historical_links[\"始发分拣中心\"].map(node_indices).values,\n",
    "        historical_links[\"到达分拣中心\"].map(node_indices).values,\n",
    "    ],\n",
    "    dtype=torch.long,\n",
    ")\n",
    "\n",
    "# 节点特征：使用节点的总货物量作为特征\n",
    "node_features = torch.zeros(len(all_nodes), 1)\n",
    "for node in all_nodes:\n",
    "    total_volume = historical_volumes[historical_volumes[\"分拣中心\"] == node][\n",
    "        \"货量\"\n",
    "    ].sum()\n",
    "    node_features[node_indices[node], 0] = total_volume\n",
    "\n",
    "# 目标值和训练掩码\n",
    "data_y = torch.zeros(len(all_nodes), 1)\n",
    "train_mask = torch.zeros(len(all_nodes), dtype=torch.bool)\n",
    "# 示例：我们使用历史数据中的货物量总和最高的节点作为训练节点\n",
    "top_nodes = historical_volumes.groupby(\"分拣中心\")[\"货量\"].sum().nlargest(10).index\n",
    "for node in top_nodes:\n",
    "    if node in node_indices:\n",
    "        data_y[node_indices[node]] = historical_volumes[\n",
    "            historical_volumes[\"分拣中心\"] == node\n",
    "        ][\"货量\"].sum()\n",
    "        train_mask[node_indices[node]] = True\n",
    "\n",
    "# 创建Data对象\n",
    "data = Data(x=node_features, edge_index=edge_index, y=data_y, train_mask=train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinmo19/Mathor_cup_2024/venv/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler_x = StandardScaler()\n",
    "data.x = torch.tensor(scaler_x.fit_transform(data.x), dtype=torch.float)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "data.y = torch.tensor(scaler_y.fit_transform(data.y.reshape(-1, 1)), dtype=torch.float).squeeze()\n",
    "# 假设data已经准备好\n",
    "dataset = [data]  # 如果有多个图，可以添加到列表中\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 future_links 已经加载\n",
    "future_edges = pd.read_csv(\"附件4.csv\", encoding=\"gb2312\")\n",
    "\n",
    "# 更新边索引\n",
    "future_edge_index = torch.tensor([\n",
    "    future_edges['始发分拣中心'].map(node_indices).values,\n",
    "    future_edges['到达分拣中心'].map(node_indices).values\n",
    "], dtype=torch.long)\n",
    "\n",
    "# 使用更新后的边索引创建新的图数据\n",
    "future_data = Data(x=data.x.clone(), edge_index=future_edge_index, y=data.y.clone(), train_mask=data.train_mask.clone())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 进行预测\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[119], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      4\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[0;32m----> 5\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 假设 data.edge_attr 是实际货物流量\u001b[39;00m\n\u001b[1;32m      6\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Mathor_cup_2024/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Mathor_cup_2024/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Mathor_cup_2024/venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Mathor_cup_2024/venv/lib/python3.11/site-packages/torch/nn/functional.py:3328\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m   3325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3326\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[1;32m   3327\u001b[0m     )\n\u001b[0;32m-> 3328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3329\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3330\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3333\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3334\u001b[0m     )\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out, data.edge_attr)  # 假设 data.edge_attr 是实际货物流量\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    print(f'Epoch {epoch+1}: Loss {loss:.4f}')\n",
    "    \n",
    "# 进行预测\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(data.x, data.edge_index)\n",
    "    predictions = predictions.squeeze()\n",
    "\n",
    "# 打印预测的货物流量\n",
    "edges = data.edge_index.t().cpu().numpy()\n",
    "for i, edge in enumerate(edges):\n",
    "    print(f\"SC{edge[0]} to SC{edge[1]}: {predictions[i].item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        predicted_volumes = out.squeeze()\n",
    "    return predicted_volumes\n",
    "\n",
    "# 进行预测\n",
    "predicted_volumes = predict(future_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 2:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 3:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 4:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 5:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 6:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 7:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 8:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 9:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 10:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 11:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 12:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 13:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 14:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 15:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 16:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 17:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 18:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 19:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 20:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 21:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 22:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 23:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 24:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 25:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 26:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 27:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 28:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 29:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n",
      "Day 30:\n",
      "  SC22: 1950545.75\n",
      "  SC18: 2087647.875\n",
      "  SC7: 4518155.0\n",
      "  SC15: 2719910.5\n",
      "  SC52: 2299113.75\n",
      "  SC2: 3000550.5\n",
      "  SC28: 2527367.0\n",
      "  SC19: 1921044.5\n",
      "  SC44: 3915013.25\n",
      "  SC23: 2387723.0\n",
      "  SC30: 2250427.75\n",
      "  SC36: 2561541.5\n",
      "  SC4: 3739726.0\n",
      "  SC24: 2111710.25\n",
      "  SC8: 4246687.0\n",
      "  SC63: 2402423.5\n",
      "  SC51: 3102367.75\n",
      "  SC55: 2301866.0\n",
      "  SC43: 2306359.0\n",
      "  SC48: 2554926.5\n",
      "  SC38: 2276400.0\n",
      "  SC32: 2366624.5\n",
      "  SC21: 2426011.0\n",
      "  SC12: 2230772.5\n",
      "  SC27: 2253245.5\n",
      "  SC16: 2404737.25\n",
      "  SC29: 2033294.875\n",
      "  SC56: 2462483.0\n",
      "  SC37: 2551911.5\n",
      "  SC54: 1763179.25\n",
      "  SC58: 2549785.0\n",
      "  SC1: 3961780.0\n",
      "  SC25: 4288059.5\n",
      "  SC5: 4148125.0\n",
      "  SC66: 2550920.0\n",
      "  SC60: 3292059.5\n",
      "  SC68: 2509217.5\n",
      "  SC10: 3736438.5\n",
      "  SC61: 2544329.5\n",
      "  SC26: 2240065.5\n",
      "  SC41: 2453746.5\n",
      "  SC39: 2398380.5\n",
      "  SC3: 2059235.25\n",
      "  SC9: 2310365.5\n",
      "  SC53: 2085295.875\n",
      "  SC34: 1805969.5\n",
      "  SC31: 2310365.5\n",
      "  SC35: 2059235.25\n",
      "  SC47: 4894072.0\n",
      "  SC6: 3614447.75\n",
      "  SC57: 3316262.0\n"
     ]
    }
   ],
   "source": [
    "def predict_daily_volumes(model, data, days=30):\n",
    "    model.eval()\n",
    "    daily_volumes = []\n",
    "    with torch.no_grad():\n",
    "        for day in range(days):\n",
    "            out = model(data.x, data.edge_index)\n",
    "            predicted_volumes = out.squeeze()\n",
    "            # 确保predicted_volumes是二维的，以符合scaler_y的要求\n",
    "            predicted_volumes = predicted_volumes.view(-1, 1)  # 从1D转换为2D，如果需要的话\n",
    "            predicted_volumes = scaler_y.inverse_transform(predicted_volumes.numpy()).flatten()\n",
    "            daily_volumes.append(predicted_volumes)\n",
    "    return daily_volumes\n",
    "\n",
    "# 使用函数进行预测\n",
    "future_daily_volumes = predict_daily_volumes(model, future_data, 30)\n",
    "\n",
    "# 输出预测结果\n",
    "for day, volumes in enumerate(future_daily_volumes, 1):\n",
    "    print(f\"Day {day}:\")\n",
    "    for i, volume in enumerate(volumes):\n",
    "        print(f\"  {all_nodes[i]}: {volume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale: [1947743.50687798]\n",
      "Mean: [933516.]\n"
     ]
    }
   ],
   "source": [
    "# 检查 scaler_y 的 scale_ 和 mean_ 属性\n",
    "print(\"Scale:\", scaler_y.scale_)\n",
    "print(\"Mean:\", scaler_y.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y values: 分拣中心\n",
      "SC60    6975702\n",
      "SC47    5603084\n",
      "SC1     5445878\n",
      "SC25    5151430\n",
      "SC8     5019169\n",
      "SC7     4738272\n",
      "SC53    3944687\n",
      "SC10    3806786\n",
      "SC6     3510835\n",
      "SC35    3413473\n",
      "Name: 货量, dtype: int64\n",
      "Standardized y values: tensor([1.9534, 2.0976, 2.3167, 2.1655, 3.1021, 1.4752, 1.5460, 1.2732, 2.3974,\n",
      "        1.3232])\n"
     ]
    }
   ],
   "source": [
    "print(\"Original y values:\", historical_volumes.groupby(\"分拣中心\")[\"货量\"].sum().nlargest(10))\n",
    "print(\"Standardized y values:\", data.y[data.train_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n",
      "Model raw output: tensor([0.5222, 0.5925, 1.8404, 0.9172, 0.7011, 1.0612, 0.8183, 0.5070, 1.5307,\n",
      "        0.7466, 0.6761, 0.8359, 1.4407, 0.6049, 1.7010, 0.7542, 1.1135, 0.7025,\n",
      "        0.7048, 0.8325, 0.6895, 0.7358, 0.7663, 0.6660, 0.6776, 0.7553, 0.5646,\n",
      "        0.7850, 0.8309, 0.4260, 0.8298, 1.5548, 1.7223, 1.6504, 0.8304, 1.2109,\n",
      "        0.8090, 1.4391, 0.8270, 0.6708, 0.7805, 0.7521, 0.5780, 0.7069, 0.5913,\n",
      "        0.4479, 0.7069, 0.5780, 2.0334, 1.3764, 1.2233])\n"
     ]
    }
   ],
   "source": [
    "def predict_daily_volumes(model, data, days=30):\n",
    "    model.eval()\n",
    "    daily_volumes = []\n",
    "    with torch.no_grad():\n",
    "        for day in range(days):\n",
    "            out = model(data.x, data.edge_index)\n",
    "            predicted_volumes = out.squeeze()\n",
    "            print(\"Model raw output:\", predicted_volumes)  # 打印原始模型输出观察\n",
    "            predicted_volumes = scaler_y.inverse_transform(predicted_volumes.view(-1, 1)).flatten()\n",
    "            daily_volumes.append(predicted_volumes)\n",
    "    return daily_volumes\n",
    "\n",
    "# 调用函数进行预测\n",
    "future_daily_volumes = predict_daily_volumes(model, future_data, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 2:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 3:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 4:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 5:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 6:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 7:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 8:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 9:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 10:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 11:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 12:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 13:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 14:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 15:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 16:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 17:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 18:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 19:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 20:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 21:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 22:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 23:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 24:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 25:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 26:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 27:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 28:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 29:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n",
      "Day 30:\n",
      "  SC22: 1.39\n",
      "  SC18: 1.64\n",
      "  SC7: 6.11\n",
      "  SC15: 2.81\n",
      "  SC52: 2.03\n",
      "  SC2: 3.32\n",
      "  SC28: 2.45\n",
      "  SC19: 1.34\n",
      "  SC44: 5.00\n",
      "  SC23: 2.19\n",
      "  SC30: 1.94\n",
      "  SC36: 2.51\n",
      "  SC4: 4.68\n",
      "  SC24: 1.69\n",
      "  SC8: 5.61\n",
      "  SC63: 2.22\n",
      "  SC51: 3.51\n",
      "  SC55: 2.04\n",
      "  SC43: 2.05\n",
      "  SC48: 2.50\n",
      "  SC38: 1.99\n",
      "  SC32: 2.16\n",
      "  SC21: 2.27\n",
      "  SC12: 1.91\n",
      "  SC27: 1.95\n",
      "  SC16: 2.23\n",
      "  SC29: 1.54\n",
      "  SC56: 2.33\n",
      "  SC37: 2.50\n",
      "  SC54: 1.05\n",
      "  SC58: 2.49\n",
      "  SC1: 5.09\n",
      "  SC25: 5.69\n",
      "  SC5: 5.43\n",
      "  SC66: 2.49\n",
      "  SC60: 3.86\n",
      "  SC68: 2.42\n",
      "  SC10: 4.67\n",
      "  SC61: 2.48\n",
      "  SC26: 1.92\n",
      "  SC41: 2.32\n",
      "  SC39: 2.21\n",
      "  SC3: 1.59\n",
      "  SC9: 2.05\n",
      "  SC53: 1.64\n",
      "  SC34: 1.12\n",
      "  SC31: 2.05\n",
      "  SC35: 1.59\n",
      "  SC47: 6.80\n",
      "  SC6: 4.45\n",
      "  SC57: 3.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 节点特征和目标值的 MinMax 缩放\n",
    "minmax_scaler_x = MinMaxScaler()\n",
    "data.x = torch.tensor(minmax_scaler_x.fit_transform(data.x), dtype=torch.float)\n",
    "\n",
    "minmax_scaler_y = MinMaxScaler()\n",
    "data.y = torch.tensor(minmax_scaler_y.fit_transform(data.y.reshape(-1, 1)), dtype=torch.float).squeeze()\n",
    "\n",
    "# 在预测函数中使用 MinMaxScaler 进行逆变换\n",
    "def predict_daily_volumes(model, data, days=30):\n",
    "    model.eval()\n",
    "    daily_volumes = []\n",
    "    with torch.no_grad():\n",
    "        for day in range(days):\n",
    "            out = model(data.x, data.edge_index)\n",
    "            predicted_volumes = out.squeeze()\n",
    "            # 使用 MinMaxScaler 逆变换\n",
    "            predicted_volumes = minmax_scaler_y.inverse_transform(predicted_volumes.view(-1, 1)).flatten()\n",
    "            daily_volumes.append(predicted_volumes)\n",
    "    return daily_volumes\n",
    "\n",
    "# 调用函数进行预测\n",
    "future_daily_volumes = predict_daily_volumes(model, future_data, 30)\n",
    "\n",
    "# 输出预测结果\n",
    "for day, volumes in enumerate(future_daily_volumes, 1):\n",
    "    print(f\"Day {day}:\")\n",
    "    for i, volume in enumerate(volumes):\n",
    "        print(f\"  {all_nodes[i]}: {volume:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
